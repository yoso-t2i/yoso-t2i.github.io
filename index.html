<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>YOSO</title>
<link href="./yoso_files/style.css" rel="stylesheet">
<script type="text/javascript" src="./yoso_files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./yoso_files/jquery.js"></script>


<style>
  p.serif{
    font-family:"Times New Roman", Times, serif;
  }
  p.sansserif{
    font-family: Arial, Helvetica, sans-serif;
  }
</style>
  
</head>

<body>
<div class="content">
  <h1><strong>You Only Sample Once: Taming One-Step Text-To-Image Synthesis by Self-Cooperative Diffusion GANs</strong>&nbsp;&nbsp;</h1>
  <p id="authors" class="serif">
    <a href="https://scholar.google.com/citations?user=9VfuwdsAAAAJ&hl=zh-CN&oi=ao">Yihong Luo<sup>1,2<dag></sup></a>
    <a href="https://scholar.google.com/citations?hl=zh-CN&user=FnznrkIAAAAJ">Xiaolong Chen<sup>1</sup></a>
    <a href="https://sites.google.com/view/jtang">Jing Tang<sup>1,2✝<dag></sup></a>
    <br>
    <a style="font-size: 0.7em"><sup>✝</sup>Corresponding Author.</a>
    <br>
    <span style="font-size: 0.8em; margin-top: 0.5em">
      <a><sup>1</sup>The Hong Kong University of Science and Technology (Guangzhou)</a>
      <br>
      <a><sup>2</sup>The Hong Kong University of Science and Technology</a>
    </span>
  </p>

  <font size="+1">
    <p style="text-align: center;" class="serif">
      <a href="https://www.arxiv.org/abs/2403.12931" target="_blank" style="font-weight: bold;">[Paper]</a>&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://github.com/Luo-Yihong/YOSO" target="_blank" style="font-weight: bold;">[Github]</a>&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="#bibtex" style="font-weight: bold;">[BibTeX]</a>
    </p><br>
  </font>

  <img class="summary-img" src="figs/yoso.png" style="width:100%;"> 

<!-- 
  <div class="row">
    <div class="col">
      <video class="clickplay" width="100%">
        <source src="animations/teaser/ani_01.mp4" type="video/mp4">
      </video>
    </div>
    <div class="col">
      <video class="clickplay" width="100%">
        <source src="animations/teaser/ani_02.mp4" type="video/mp4">
      </video>
    </div>
    <div class="col">
      <video class="clickplay" width="100%">
        <source src="animations/teaser/ani_03.mp4" type="video/mp4">
      </video>
    </div>
  </div>

  <div class="row">
    <div class="col">
      <video class="clickplay" width="100%">
        <source src="animations/teaser/ani_04.mp4" type="video/mp4">
      </video>
    </div>
    <div class="col">
      <video class="clickplay" width="100%">
        <source src="animations/teaser/ani_05.mp4" type="video/mp4">
      </video>
    </div>
    <div class="col">
      <video class="clickplay" width="100%">
        <source src="animations/teaser/ani_06.mp4" type="video/mp4">
      </video>
    </div>
  </div> -->

</div>

<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Abstract</p>
  <p style="font-size: 1.2em" class="serif">
    We introduce <b>YOSO</b>, a novel generative model designed for rapid, scalable, and high-fidelity one-step image synthesis. This is achieved by integrating the diffusion process with GANs. Specifically, we smooth the distribution by the denoising generator itself, performing self-cooperative learning. We show that our method can serve as a one-step generation model training from scratch with competitive performance. Moreover, we show that our method can be extended to finetune pre-trained text-to-image diffusion for high-quality one-step text-to-image synthesis even with LoRA fine-tuning. In particular, we provide the first diffusion transformer that can generate images in one step trained on 512 resolution, with the capability of adapting to 1024 resolution without explicit training.
  </p>
</div>

<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Methodology</p>
  <p style="font-size: 1.2em" class="serif">
    Our approach combines diffusion processes with GANs, bridging the gap between the two models and leveraging the distinct advantages of each: it can be versatile for various downstream tasks while also boasting the advantage of rapid generation. To address the typical instability in GAN training, we do not directly use real data as ground truth for training. Specifically, we believe that the quality of images denoised by generator from highly-noised images will be lower than that from lowly-noised images. Therefore, we use the former as the fake images for GAN training and the latter as the real images. This design significantly stabilizes our GAN training process, and since the training is still defined on clean samples, it successfully acquires the ability to perform one-step generation.

    Benefiting from the proposed design, our method possesses the following advantages: <br>
    1. A stable training process, <br>
    2. Extremely fast high-quality one-step generation, <br>
    3. Broad prospects for application in various downstream tasks.
  </p>
  <img class="summary-img" src="figs/frame.png" style="width:100%;"> <br>
</div>


<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Qualitative Comparison</p>
  <p style="font-size: 1.2em" class="serif">
    To provide more comprehensive insight in understanding the performance of our YOSO, we further provide the qualitative comparison here. The comparison shows that: YOSO clearly beats SD-Turbo in terms of both image quality and prompt alignment. Specifically, the samples by SD-Turbo are blurry to some extent. Although SDXL-Turbo with a larger model size improves the sample quality compared to SD-Turbo, we find that the samples by YOSO is sharper than SDXL-Turbo to some extent, and we do better in prompt alignment. For example, in the first row related to a Pikachu, both SD-Turbo and SDXL-Turbo cannot follow the prompt to generate angry expressions and red eyes, while YOSO follows the prompt more precisely. Moreover, it is important to highlight that SDXL-Turbo seems to have a serious issue in <i>mode collapse</i>. Specifically, the first row and third row show that samples by SDXL-Turbo are really close to each other. This indicates that SDXL-Turbo performs poorly in mode cover. Overall, based on quantitative and qualitative results, we conclude that our YOSO is better in sample quality, prompt alignment, and mode cover compared with the state-of-the-art one-step text-to-image models (i.e., SD-Turbo and SDXL-Turbo).

    <!-- We recommend users visit our github repo and play with our pre-trained weights. -->
  </p>
  <img class="summary-img" src="figs/comparison.svg" style="width:100%;"> 
</div>

<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Down-stream Tasks</p>
  <p style="font-size: 1.2em" class="serif">

  </p>
  <img class="summary-img" src="figs/lora_app.png" style="width:80%;"> 
</div>

</div>

<div class="content" id="bibtex">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">BibTeX</p>
  <code> @misc{luo2024sample,<br>
  &nbsp;&nbsp; title={You Only Sample Once: Taming One-Step Text-To-Image Synthesis by Self-Cooperative Diffusion GANs},<br>
  &nbsp;&nbsp;author={Yihong Luo and Xiaolong Chen and Jing Tang},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2403.12931},<br>
  &nbsp;&nbsp;year={2024},<br>
  &nbsp;&nbsp;archivePrefix={arXiv},<br>
  &nbsp;&nbsp;primaryClass={cs.CV}<br>
  } </code> 
</div>

<div class="content">
  <p class="serif">
    Project page template is borrowed from <a href="https://dreambooth.github.io/">DreamBooth</a>.
  </p>
</div>

</body>

<script>
var videos = document.getElementsByClassName("clickplay");
for (var i = 0; i < videos.length; i++) {
  videos[i].addEventListener("click", function() {
    this.play();
  });
  videos[i].addEventListener("ended", function() {
    this.pause();
    this.currentTime = 0;
  });
}
</script>

</html>
